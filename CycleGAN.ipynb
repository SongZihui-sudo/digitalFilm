{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from torchvision.utils import save_image\n",
    "import itertools\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePairDataset(Dataset):\n",
    "    def __init__(self, folder1, folder2, pairs, transform=None, enhance_transform_1 = None, \n",
    "                 enhance_transform_2 = None, enhance_transform_3 = None):\n",
    "        self.folder1 = folder1\n",
    "        self.folder2 = folder2\n",
    "        self.pairs = pairs\n",
    "        self.transform = transform\n",
    "        self.enhance_transform_1 = enhance_transform_1\n",
    "        self.enhance_transform_2 = enhance_transform_2\n",
    "        self.enhance_transform_3 = enhance_transform_3\n",
    "        self.image_pairs = self.read_image_pairs()\n",
    "\n",
    "    def read_image_pairs(self):\n",
    "      image_pairs = []\n",
    "      for image_pair in tqdm.tqdm(self.pairs):\n",
    "        img1_path = os.path.join(self.folder1, image_pair[0])\n",
    "        img2_path = os.path.join(self.folder2, image_pair[1])\n",
    "        img1 = Image.open(img1_path).convert(\"RGB\")\n",
    "        img2 = Image.open(img2_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        image_pairs.append((img1, img2))\n",
    "      return image_pairs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      return self.image_pairs[idx][0], self.image_pairs[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 归一化\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "digital_dir = \"D:/data/富士c200/数码\"\n",
    "film_dir = \"D:/data/富士c200/样片\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(image1_dir, imgage2_dir):\n",
    "    # Get the list of files in both folders\n",
    "    images1 = sorted(os.listdir(image1_dir))\n",
    "    images2 = sorted(os.listdir(imgage2_dir))\n",
    "    \n",
    "    print(len(images1))\n",
    "    print(len(images2))\n",
    "\n",
    "    # Ensure the number of files match\n",
    "    if len(images1) != len(images2):\n",
    "        raise ValueError(\"The two folders must have the same number of images.\")\n",
    "\n",
    "    # Create pairs of images (file1, file2)\n",
    "    pairs = list(zip(images1, images2))\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n",
      "230\n"
     ]
    }
   ],
   "source": [
    "pairs = make_pairs(digital_dir, film_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:18<00:00, 12.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = ImagePairDataset(digital_dir, film_dir, pairs, transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1. 定义生成器 (ResNet Generator)\n",
    "# ============================\n",
    "class ResNetGenerator(nn.Module):\n",
    "    def __init__(self, input_nc=3, output_nc=3, num_residual_blocks=9):\n",
    "        super(ResNetGenerator, self).__init__()\n",
    "        \n",
    "        # 使用预训练的 ResNet-18\n",
    "        resnet = models.resnet18()\n",
    "        resnet.load_state_dict(torch.load(\"./resnet18-5c106cde.pth\", weights_only=False))\n",
    "        \n",
    "        # 编码器：获取 ResNet 的各层输出，构造 U-Net 风格的跳跃连接\n",
    "        self.input_conv = nn.Sequential(\n",
    "            resnet.conv1,   # 输出: 64通道, H/2 x W/2\n",
    "            resnet.bn1,\n",
    "            resnet.relu\n",
    "        )\n",
    "        self.maxpool = resnet.maxpool   # H/4 x W/4\n",
    "        self.encoder1 = resnet.layer1   # 输出: 64通道, H/4 x W/4\n",
    "        self.encoder2 = resnet.layer2   # 输出: 128通道, H/8 x W/8\n",
    "        self.encoder3 = resnet.layer3   # 输出: 256通道, H/16 x W/16\n",
    "        self.encoder4 = resnet.layer4   # 输出: 512通道, H/32 x W/32\n",
    "\n",
    "        # 解码器：使用上采样（双线性插值）+ 卷积来恢复细节，并与跳跃连接特征融合\n",
    "        self.up1 = self._up_block(512, 256)  # H/32 -> H/16\n",
    "        self.up2 = self._up_block(512, 128)  # 256(from up1)+256(encoder3) -> H/16 -> H/8\n",
    "        self.up3 = self._up_block(256, 64)   # 128(from up2)+128(encoder2) -> H/8 -> H/4\n",
    "        self.up4 = self._up_block(128, 64)   # 64(from up3)+64(encoder1) -> H/4 -> H/2\n",
    "        self.up5 = self._up_block(128, 64)   # 64(from up4)+64(from input_conv) -> H/2 -> H\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, output_nc, kernel_size=1)\n",
    "\n",
    "    def _up_block(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        上采样块：先上采样（双线性），再卷积+ReLU\n",
    "        \"\"\"\n",
    "        block = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        return block\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x.shape[2]\n",
    "        w = x.shape[3]\n",
    "\n",
    "        # 编码器部分\n",
    "        x0 = self.input_conv(x)       # x0: [B, 64, H/2, W/2]\n",
    "        x1 = self.maxpool(x0)         # x1: [B, 64, H/4, W/4]\n",
    "        x1 = self.encoder1(x1)        # x1: [B, 64, H/4, W/4]\n",
    "        x2 = self.encoder2(x1)        # x2: [B, 128, H/8, W/8]\n",
    "        x3 = self.encoder3(x2)        # x3: [B, 256, H/16, W/16]\n",
    "        x4 = self.encoder4(x3)        # x4: [B, 512, H/32, W/32]\n",
    "\n",
    "        # 解码器部分：逐步上采样并与编码器对应层特征拼接\n",
    "        d1 = self.up1(x4)             # d1: [B, 256, H/16, W/16]\n",
    "        # 拼接 x3 (256通道)\n",
    "        d1 = torch.cat([d1, x3], dim=1)  # [B, 256+256=512, H/16, W/16]\n",
    "\n",
    "        d2 = self.up2(d1)             # d2: [B, 128, H/8, W/8]\n",
    "        # 拼接 x2 (128通道)\n",
    "        d2 = torch.cat([d2, x2], dim=1)  # [B, 128+128=256, H/8, W/8]\n",
    "\n",
    "        d3 = self.up3(d2)             # d3: [B, 64, H/4, W/4]\n",
    "        # 拼接 x1 (64通道)\n",
    "        d3 = torch.cat([d3, x1], dim=1)  # [B, 64+64=128, H/4, W/4]\n",
    "\n",
    "        d4 = self.up4(d3)             # d4: [B, 64, H/2, W/2]\n",
    "        # 拼接 x0 (64通道)\n",
    "        d4 = torch.cat([d4, x0], dim=1)  # [B, 64+64=128, H/2, W/2]\n",
    "\n",
    "        d5 = self.up5(d4)             # d5: [B, 64, H, W]\n",
    "\n",
    "        out = self.final_conv(d5)     # out: [B, output_nc, H, W]\n",
    "        out = torch.tanh(out)\n",
    "\n",
    "        out = torch.nn.functional.interpolate(out, size = (h, w), mode=\"bilinear\")\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 3. 定义判别器 (PatchGAN)\n",
    "# ============================\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc):\n",
    "        \"\"\"\n",
    "        使用 70×70 PatchGAN 判别器\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        model = [\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        in_channels = 64\n",
    "        out_channels = in_channels * 2\n",
    "        # 增加几层卷积\n",
    "        for _ in range(3):\n",
    "            model += [\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "            out_channels = in_channels * 2\n",
    "\n",
    "        # 最后一层卷积\n",
    "        model += [\n",
    "            nn.Conv2d(in_channels, 1, kernel_size=4, padding=1)\n",
    "        ]\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 4. 定义 CycleGAN 模型类（加入 AMP 支持）\n",
    "# ============================\n",
    "class CycleGAN:\n",
    "    def __init__(self, device, G_AB = \"\", G_BA = \"\", D_A = \"\", D_B = \"\"):\n",
    "        self.device = device\n",
    "\n",
    "        # 初始化两个生成器：G_AB（数码→胶片）、G_BA（胶片→数码）\n",
    "        self.G_AB = ResNetGenerator(3, 3).to(device)\n",
    "        if G_AB != \"\":\n",
    "            self.G_AB.load_state_dict(torch.load(G_AB))\n",
    "        self.G_BA = ResNetGenerator(3, 3).to(device)\n",
    "        if G_BA != \"\":\n",
    "            self.G_BA.load_state_dict(torch.load(G_BA))\n",
    "        # 初始化两个判别器：D_A（判别真实数码图像）、D_B（判别真实胶片图像）\n",
    "        self.D_A = Discriminator(3).to(device)\n",
    "        if D_A != \"\":\n",
    "            self.D_A.load_state_dict(torch.load(D_A))\n",
    "        self.D_B = Discriminator(3).to(device)\n",
    "        if D_B != \"\":\n",
    "            self.D_B.load_state_dict(torch.load(D_B))\n",
    "        # 定义损失函数：对抗损失、循环一致性损失、身份损失\n",
    "        self.criterion_GAN = nn.MSELoss().to(device)\n",
    "        self.criterion_cycle = nn.L1Loss().to(device)\n",
    "        self.criterion_identity = nn.L1Loss().to(device)\n",
    "\n",
    "        # 优化器（两个生成器共用一个优化器）\n",
    "        self.optimizer_G = optim.Adam(itertools.chain(self.G_AB.parameters(), self.G_BA.parameters()),\n",
    "                                    lr=0.0002, betas=(0.5, 0.999))\n",
    "        self.optimizer_D_A = optim.Adam(self.D_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        self.optimizer_D_B = optim.Adam(self.D_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "        # 创建 AMP GradScaler 对象\n",
    "        self.scaler_G = torch.amp.GradScaler(device=device)\n",
    "        self.scaler_D_A = torch.amp.GradScaler(device=device)\n",
    "        self.scaler_D_B = torch.amp.GradScaler(device=device)\n",
    "\n",
    "    def set_input(self, real_A, real_B):\n",
    "        self.real_A = real_A.to(self.device)\n",
    "        self.real_B = real_B.to(self.device)\n",
    "\n",
    "    def forward(self):\n",
    "        # A→B→A\n",
    "        self.fake_B = self.G_AB(self.real_A)\n",
    "        self.rec_A = self.G_BA(self.fake_B)\n",
    "        # B→A→B\n",
    "        self.fake_A = self.G_BA(self.real_B)\n",
    "        self.rec_B = self.G_AB(self.fake_A)\n",
    "\n",
    "    def backward_G(self):\n",
    "        # 身份损失：要求生成器在目标域图像上保持不变\n",
    "        self.idt_A = self.G_BA(self.real_A)\n",
    "        self.loss_idt_A = self.criterion_identity(self.idt_A, self.real_A) * 5.0\n",
    "        self.idt_B = self.G_AB(self.real_B)\n",
    "        self.loss_idt_B = self.criterion_identity(self.idt_B, self.real_B) * 5.0\n",
    "\n",
    "        # 对抗损失\n",
    "        pred_fake_B = self.D_B(self.fake_B)\n",
    "        target_real = torch.ones_like(pred_fake_B, device=self.device)\n",
    "        loss_GAN_AB = self.criterion_GAN(pred_fake_B, target_real)\n",
    "\n",
    "        pred_fake_A = self.D_A(self.fake_A)\n",
    "        target_real = torch.ones_like(pred_fake_A, device=self.device)\n",
    "        loss_GAN_BA = self.criterion_GAN(pred_fake_A, target_real)\n",
    "\n",
    "        # 循环一致性损失\n",
    "        loss_cycle_A = self.criterion_cycle(self.rec_A, self.real_A) * 10.0\n",
    "        loss_cycle_B = self.criterion_cycle(self.rec_B, self.real_B) * 10.0\n",
    "\n",
    "        # 总生成器损失（仅保存，不调用 backward）\n",
    "        self.loss_G = self.loss_idt_A + self.loss_idt_B + loss_GAN_AB + loss_GAN_BA + loss_cycle_A + loss_cycle_B\n",
    "\n",
    "    def backward_D_basic(self, netD, real, fake):\n",
    "        target_real = torch.ones_like(netD(real), device=self.device)\n",
    "        target_fake = torch.zeros_like(netD(fake.detach()), device=self.device)\n",
    "        loss_real = self.criterion_GAN(netD(real), target_real)\n",
    "        loss_fake = self.criterion_GAN(netD(fake.detach()), target_fake)\n",
    "        loss_D = (loss_real + loss_fake) * 0.5\n",
    "        return loss_D\n",
    "\n",
    "    def backward_D_A(self):\n",
    "        self.loss_D_A = self.backward_D_basic(self.D_A, self.real_A, self.fake_A)\n",
    "\n",
    "    def backward_D_B(self):\n",
    "        self.loss_D_B = self.backward_D_basic(self.D_B, self.real_B, self.fake_B)\n",
    "\n",
    "    def optimize_parameters(self):\n",
    "        # --------------------------\n",
    "        # 更新生成器\n",
    "        # --------------------------\n",
    "        self.optimizer_G.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            self.forward()       # 生成 fake_B、fake_A、rec_A、rec_B\n",
    "            self.backward_G()    # 计算 self.loss_G\n",
    "        self.scaler_G.scale(self.loss_G).backward()\n",
    "        self.scaler_G.step(self.optimizer_G)\n",
    "        self.scaler_G.update()\n",
    "\n",
    "        # --------------------------\n",
    "        # 更新判别器 D_A\n",
    "        # --------------------------\n",
    "        self.optimizer_D_A.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            self.backward_D_A()  # 计算 self.loss_D_A\n",
    "        self.scaler_D_A.scale(self.loss_D_A).backward()\n",
    "        self.scaler_D_A.step(self.optimizer_D_A)\n",
    "        self.scaler_D_A.update()\n",
    "\n",
    "        # --------------------------\n",
    "        # 更新判别器 D_B\n",
    "        # --------------------------\n",
    "        self.optimizer_D_B.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            self.backward_D_B()  # 计算 self.loss_D_B\n",
    "        self.scaler_D_B.scale(self.loss_D_B).backward()\n",
    "        self.scaler_D_B.step(self.optimizer_D_B)\n",
    "        self.scaler_D_B.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 5. 训练循环函数\n",
    "# ============================\n",
    "def train(cyclegan, dataloader, num_epochs=200, save_interval=10, pre_epoch = 0):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data_A, data_B) in enumerate(dataloader):\n",
    "            real_A = data_A\n",
    "            real_B = data_B\n",
    "            cyclegan.set_input(real_A, real_B)\n",
    "            cyclegan.optimize_parameters()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1 + pre_epoch}/{num_epochs + pre_epoch}] Batch [{i}] | \"\n",
    "                      f\"Loss_G: {cyclegan.loss_G.item():.4f} | \"\n",
    "                      f\"Loss_D_A: {cyclegan.loss_D_A.item():.4f} | \"\n",
    "                      f\"Loss_D_B: {cyclegan.loss_D_B.item():.4f}\")\n",
    "\n",
    "        # 定期保存输出结果及模型\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            os.makedirs(\"output\", exist_ok=True)\n",
    "            with torch.no_grad():\n",
    "                fake_B = cyclegan.G_AB(real_A.cuda())\n",
    "                fake_A = cyclegan.G_BA(real_B.cuda())\n",
    "            save_image(fake_B, f\"output/fake_B_epoch_{epoch+1+pre_epoch}.png\", normalize=True)\n",
    "            save_image(fake_A, f\"output/fake_A_epoch_{epoch+1+pre_epoch}.png\", normalize=True)\n",
    "            torch.save(cyclegan.G_AB.state_dict(), f\"output/G_AB_epoch_{epoch+1+pre_epoch}.pth\")\n",
    "            torch.save(cyclegan.G_BA.state_dict(), f\"output/G_BA_epoch_{epoch+1+pre_epoch}.pth\")\n",
    "            torch.save(cyclegan.D_A.state_dict(), f\"output/D_A_epoch_{epoch+1+pre_epoch}.pth\")\n",
    "            torch.save(cyclegan.D_B.state_dict(), f\"output/D_B_epoch_{epoch+1+pre_epoch}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 CycleGAN 模型\n",
    "cyclegan = CycleGAN(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13012\\3913547452.py:34: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler_G = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13012\\3913547452.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler_D_A = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13012\\3913547452.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler_D_B = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "cyclegan = CycleGAN(device, G_AB=\"output/G_AB_epoch_200.pth\", \n",
    "                    G_BA=\"output/G_BA_epoch_200.pth\", \n",
    "                    D_A=\"output/D_A_epoch_200.pth\", \n",
    "                    D_B=\"output/D_B_epoch_200.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11284\\251787170.py:92: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11284\\251787170.py:103: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11284\\251787170.py:113: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [101/200] Batch [0] | Loss_G: 3.7649 | Loss_D_A: 0.0855 | Loss_D_B: 0.0368\n",
      "Epoch [102/200] Batch [0] | Loss_G: 3.3838 | Loss_D_A: 0.2978 | Loss_D_B: 0.0511\n",
      "Epoch [103/200] Batch [0] | Loss_G: 3.8999 | Loss_D_A: 0.0991 | Loss_D_B: 0.0593\n",
      "Epoch [104/200] Batch [0] | Loss_G: 3.2847 | Loss_D_A: 0.0634 | Loss_D_B: 0.3842\n",
      "Epoch [105/200] Batch [0] | Loss_G: 3.6745 | Loss_D_A: 0.1360 | Loss_D_B: 0.0629\n",
      "Epoch [106/200] Batch [0] | Loss_G: 4.5686 | Loss_D_A: 0.1052 | Loss_D_B: 0.0489\n",
      "Epoch [107/200] Batch [0] | Loss_G: 4.0143 | Loss_D_A: 0.0694 | Loss_D_B: 0.0495\n",
      "Epoch [108/200] Batch [0] | Loss_G: 4.5522 | Loss_D_A: 0.1479 | Loss_D_B: 0.0621\n",
      "Epoch [109/200] Batch [0] | Loss_G: 4.3564 | Loss_D_A: 0.0938 | Loss_D_B: 0.0562\n",
      "Epoch [110/200] Batch [0] | Loss_G: 4.1930 | Loss_D_A: 0.0707 | Loss_D_B: 0.0688\n",
      "Epoch [111/200] Batch [0] | Loss_G: 4.2989 | Loss_D_A: 0.0784 | Loss_D_B: 0.0361\n",
      "Epoch [112/200] Batch [0] | Loss_G: 4.3928 | Loss_D_A: 0.1529 | Loss_D_B: 0.0966\n",
      "Epoch [113/200] Batch [0] | Loss_G: 4.0563 | Loss_D_A: 0.1288 | Loss_D_B: 0.0445\n",
      "Epoch [114/200] Batch [0] | Loss_G: 4.6539 | Loss_D_A: 0.0897 | Loss_D_B: 0.0318\n",
      "Epoch [115/200] Batch [0] | Loss_G: 5.4845 | Loss_D_A: 0.0930 | Loss_D_B: 0.2309\n",
      "Epoch [116/200] Batch [0] | Loss_G: 4.0543 | Loss_D_A: 0.1220 | Loss_D_B: 0.0366\n",
      "Epoch [117/200] Batch [0] | Loss_G: 5.3534 | Loss_D_A: 0.0963 | Loss_D_B: 0.0416\n",
      "Epoch [118/200] Batch [0] | Loss_G: 4.0816 | Loss_D_A: 0.0981 | Loss_D_B: 0.0464\n",
      "Epoch [119/200] Batch [0] | Loss_G: 3.9948 | Loss_D_A: 0.1059 | Loss_D_B: 0.0761\n",
      "Epoch [120/200] Batch [0] | Loss_G: 5.0650 | Loss_D_A: 0.1281 | Loss_D_B: 0.0581\n",
      "Epoch [121/200] Batch [0] | Loss_G: 4.1829 | Loss_D_A: 0.2672 | Loss_D_B: 0.0924\n",
      "Epoch [122/200] Batch [0] | Loss_G: 4.6414 | Loss_D_A: 0.0825 | Loss_D_B: 0.0303\n",
      "Epoch [123/200] Batch [0] | Loss_G: 4.5211 | Loss_D_A: 0.0776 | Loss_D_B: 0.0514\n",
      "Epoch [124/200] Batch [0] | Loss_G: 3.5749 | Loss_D_A: 0.0805 | Loss_D_B: 0.1694\n",
      "Epoch [125/200] Batch [0] | Loss_G: 4.7581 | Loss_D_A: 0.1146 | Loss_D_B: 0.0971\n",
      "Epoch [126/200] Batch [0] | Loss_G: 4.0497 | Loss_D_A: 0.0721 | Loss_D_B: 0.0562\n",
      "Epoch [127/200] Batch [0] | Loss_G: 3.4226 | Loss_D_A: 0.0942 | Loss_D_B: 0.0883\n",
      "Epoch [128/200] Batch [0] | Loss_G: 3.7877 | Loss_D_A: 0.0766 | Loss_D_B: 0.0351\n",
      "Epoch [129/200] Batch [0] | Loss_G: 4.5348 | Loss_D_A: 0.1998 | Loss_D_B: 0.0756\n",
      "Epoch [130/200] Batch [0] | Loss_G: 4.3275 | Loss_D_A: 0.0744 | Loss_D_B: 0.0300\n",
      "Epoch [131/200] Batch [0] | Loss_G: 3.9628 | Loss_D_A: 0.0802 | Loss_D_B: 0.0296\n",
      "Epoch [132/200] Batch [0] | Loss_G: 4.4693 | Loss_D_A: 0.1018 | Loss_D_B: 0.0590\n",
      "Epoch [133/200] Batch [0] | Loss_G: 4.1093 | Loss_D_A: 0.3146 | Loss_D_B: 0.0426\n",
      "Epoch [134/200] Batch [0] | Loss_G: 3.8959 | Loss_D_A: 0.0939 | Loss_D_B: 0.0515\n",
      "Epoch [135/200] Batch [0] | Loss_G: 4.3146 | Loss_D_A: 0.0545 | Loss_D_B: 0.0218\n",
      "Epoch [136/200] Batch [0] | Loss_G: 4.3678 | Loss_D_A: 0.1749 | Loss_D_B: 0.0264\n",
      "Epoch [137/200] Batch [0] | Loss_G: 4.6648 | Loss_D_A: 0.0520 | Loss_D_B: 0.0835\n",
      "Epoch [138/200] Batch [0] | Loss_G: 3.9261 | Loss_D_A: 0.1574 | Loss_D_B: 0.0725\n",
      "Epoch [139/200] Batch [0] | Loss_G: 3.9528 | Loss_D_A: 0.0814 | Loss_D_B: 0.0357\n",
      "Epoch [140/200] Batch [0] | Loss_G: 4.4151 | Loss_D_A: 0.0621 | Loss_D_B: 0.0301\n",
      "Epoch [141/200] Batch [0] | Loss_G: 4.3816 | Loss_D_A: 0.0767 | Loss_D_B: 0.0396\n",
      "Epoch [142/200] Batch [0] | Loss_G: 4.1360 | Loss_D_A: 0.1141 | Loss_D_B: 0.2099\n",
      "Epoch [143/200] Batch [0] | Loss_G: 4.3633 | Loss_D_A: 0.0603 | Loss_D_B: 0.0558\n",
      "Epoch [144/200] Batch [0] | Loss_G: 4.3715 | Loss_D_A: 0.0631 | Loss_D_B: 0.0439\n",
      "Epoch [145/200] Batch [0] | Loss_G: 4.5221 | Loss_D_A: 0.0959 | Loss_D_B: 0.0378\n",
      "Epoch [146/200] Batch [0] | Loss_G: 4.3254 | Loss_D_A: 0.1870 | Loss_D_B: 0.0835\n",
      "Epoch [147/200] Batch [0] | Loss_G: 4.1855 | Loss_D_A: 0.0745 | Loss_D_B: 0.0536\n",
      "Epoch [148/200] Batch [0] | Loss_G: 4.6140 | Loss_D_A: 0.1695 | Loss_D_B: 0.0788\n",
      "Epoch [149/200] Batch [0] | Loss_G: 4.0147 | Loss_D_A: 0.0644 | Loss_D_B: 0.0479\n",
      "Epoch [150/200] Batch [0] | Loss_G: 4.5149 | Loss_D_A: 0.0529 | Loss_D_B: 0.0577\n",
      "Epoch [151/200] Batch [0] | Loss_G: 4.6325 | Loss_D_A: 0.0809 | Loss_D_B: 0.0587\n",
      "Epoch [152/200] Batch [0] | Loss_G: 4.6140 | Loss_D_A: 0.1005 | Loss_D_B: 0.0332\n",
      "Epoch [153/200] Batch [0] | Loss_G: 4.9247 | Loss_D_A: 0.1344 | Loss_D_B: 0.0864\n",
      "Epoch [154/200] Batch [0] | Loss_G: 6.6080 | Loss_D_A: 0.0961 | Loss_D_B: 2.0398\n",
      "Epoch [155/200] Batch [0] | Loss_G: 3.5613 | Loss_D_A: 0.0654 | Loss_D_B: 0.2543\n",
      "Epoch [156/200] Batch [0] | Loss_G: 3.5674 | Loss_D_A: 0.0789 | Loss_D_B: 0.2230\n",
      "Epoch [157/200] Batch [0] | Loss_G: 3.2344 | Loss_D_A: 0.1801 | Loss_D_B: 0.2267\n",
      "Epoch [158/200] Batch [0] | Loss_G: 3.0694 | Loss_D_A: 0.0884 | Loss_D_B: 0.2093\n",
      "Epoch [159/200] Batch [0] | Loss_G: 3.5844 | Loss_D_A: 0.0479 | Loss_D_B: 0.1996\n",
      "Epoch [160/200] Batch [0] | Loss_G: 3.6044 | Loss_D_A: 0.0864 | Loss_D_B: 0.1897\n",
      "Epoch [161/200] Batch [0] | Loss_G: 3.4814 | Loss_D_A: 0.1312 | Loss_D_B: 0.1564\n",
      "Epoch [162/200] Batch [0] | Loss_G: 3.8268 | Loss_D_A: 0.0604 | Loss_D_B: 0.1610\n",
      "Epoch [163/200] Batch [0] | Loss_G: 4.3210 | Loss_D_A: 0.0616 | Loss_D_B: 0.1842\n",
      "Epoch [164/200] Batch [0] | Loss_G: 4.6058 | Loss_D_A: 0.1738 | Loss_D_B: 0.1464\n",
      "Epoch [165/200] Batch [0] | Loss_G: 3.9992 | Loss_D_A: 0.0543 | Loss_D_B: 0.1645\n",
      "Epoch [166/200] Batch [0] | Loss_G: 3.9516 | Loss_D_A: 0.0683 | Loss_D_B: 0.1226\n",
      "Epoch [167/200] Batch [0] | Loss_G: 4.1045 | Loss_D_A: 0.0550 | Loss_D_B: 0.1075\n",
      "Epoch [168/200] Batch [0] | Loss_G: 3.9016 | Loss_D_A: 0.0910 | Loss_D_B: 0.0993\n",
      "Epoch [169/200] Batch [0] | Loss_G: 4.0892 | Loss_D_A: 0.0793 | Loss_D_B: 0.0955\n",
      "Epoch [170/200] Batch [0] | Loss_G: 4.5148 | Loss_D_A: 0.0989 | Loss_D_B: 0.0785\n",
      "Epoch [171/200] Batch [0] | Loss_G: 3.9879 | Loss_D_A: 0.0911 | Loss_D_B: 0.1216\n",
      "Epoch [172/200] Batch [0] | Loss_G: 4.1076 | Loss_D_A: 0.0651 | Loss_D_B: 0.1018\n",
      "Epoch [173/200] Batch [0] | Loss_G: 4.2380 | Loss_D_A: 0.1344 | Loss_D_B: 0.1659\n",
      "Epoch [174/200] Batch [0] | Loss_G: 4.6580 | Loss_D_A: 0.0537 | Loss_D_B: 0.0771\n",
      "Epoch [175/200] Batch [0] | Loss_G: 4.5683 | Loss_D_A: 0.0556 | Loss_D_B: 0.0605\n",
      "Epoch [176/200] Batch [0] | Loss_G: 4.5670 | Loss_D_A: 0.0441 | Loss_D_B: 0.0695\n",
      "Epoch [177/200] Batch [0] | Loss_G: 4.2700 | Loss_D_A: 0.1091 | Loss_D_B: 0.0519\n",
      "Epoch [178/200] Batch [0] | Loss_G: 4.0993 | Loss_D_A: 0.0582 | Loss_D_B: 0.0446\n",
      "Epoch [179/200] Batch [0] | Loss_G: 4.0947 | Loss_D_A: 0.1358 | Loss_D_B: 0.0317\n",
      "Epoch [180/200] Batch [0] | Loss_G: 3.7170 | Loss_D_A: 0.0578 | Loss_D_B: 0.0844\n",
      "Epoch [181/200] Batch [0] | Loss_G: 4.5471 | Loss_D_A: 0.0425 | Loss_D_B: 0.0533\n",
      "Epoch [182/200] Batch [0] | Loss_G: 3.8874 | Loss_D_A: 0.0600 | Loss_D_B: 0.0607\n",
      "Epoch [183/200] Batch [0] | Loss_G: 4.4674 | Loss_D_A: 0.0340 | Loss_D_B: 0.0753\n",
      "Epoch [184/200] Batch [0] | Loss_G: 4.5901 | Loss_D_A: 0.0681 | Loss_D_B: 0.0511\n",
      "Epoch [185/200] Batch [0] | Loss_G: 4.1918 | Loss_D_A: 0.0601 | Loss_D_B: 0.0639\n",
      "Epoch [186/200] Batch [0] | Loss_G: 4.5739 | Loss_D_A: 0.0638 | Loss_D_B: 0.0468\n",
      "Epoch [187/200] Batch [0] | Loss_G: 4.4676 | Loss_D_A: 0.0764 | Loss_D_B: 0.0966\n",
      "Epoch [188/200] Batch [0] | Loss_G: 4.2309 | Loss_D_A: 0.1866 | Loss_D_B: 0.1620\n",
      "Epoch [189/200] Batch [0] | Loss_G: 4.2021 | Loss_D_A: 0.5883 | Loss_D_B: 0.0550\n",
      "Epoch [190/200] Batch [0] | Loss_G: 4.8365 | Loss_D_A: 0.8011 | Loss_D_B: 0.0490\n",
      "Epoch [191/200] Batch [0] | Loss_G: 3.5159 | Loss_D_A: 0.2613 | Loss_D_B: 0.0734\n",
      "Epoch [192/200] Batch [0] | Loss_G: 3.5489 | Loss_D_A: 0.2454 | Loss_D_B: 0.0530\n",
      "Epoch [193/200] Batch [0] | Loss_G: 3.1599 | Loss_D_A: 0.2479 | Loss_D_B: 0.0899\n",
      "Epoch [194/200] Batch [0] | Loss_G: 3.4845 | Loss_D_A: 0.2342 | Loss_D_B: 0.0537\n",
      "Epoch [195/200] Batch [0] | Loss_G: 3.2599 | Loss_D_A: 0.2358 | Loss_D_B: 0.0504\n",
      "Epoch [196/200] Batch [0] | Loss_G: 3.5737 | Loss_D_A: 0.2327 | Loss_D_B: 0.0404\n",
      "Epoch [197/200] Batch [0] | Loss_G: 3.1598 | Loss_D_A: 0.2291 | Loss_D_B: 0.1177\n",
      "Epoch [198/200] Batch [0] | Loss_G: 3.6380 | Loss_D_A: 0.2166 | Loss_D_B: 0.0591\n",
      "Epoch [199/200] Batch [0] | Loss_G: 3.6991 | Loss_D_A: 0.2283 | Loss_D_B: 0.0498\n",
      "Epoch [200/200] Batch [0] | Loss_G: 3.5832 | Loss_D_A: 0.2225 | Loss_D_B: 0.0441\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "train(cyclegan, train_loader, num_epochs=100, pre_epoch = 100, save_interval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_1456\\3913547452.py:34: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler_G = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_1456\\3913547452.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler_D_A = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_1456\\3913547452.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler_D_B = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "cyclegan = CycleGAN(device, G_AB=\"output/G_AB_epoch_100.pth\", \n",
    "                    G_BA=\"output/G_BA_epoch_100.pth\", \n",
    "                    D_A=\"output/D_A_epoch_100.pth\", \n",
    "                    D_B=\"output/D_B_epoch_100.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 归一化\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_digital_dir = \"D:/data/柯达金200/微调数据集/测试照片\"\n",
    "test_film_dir = \"D:/data/柯达金200/微调数据集/测试胶片\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "pairs = make_pairs(test_digital_dir, test_film_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = ImagePairDataset(test_digital_dir, test_film_dir, pairs, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_1456\\1491647904.py:48: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  x = torch.nn.functional.upsample(x, size = (h, w), mode=\"bilinear\")\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.970074..0.9999682].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.99999946..0.99990547].\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# 测试阶段：展示结果\n",
    "with torch.no_grad():\n",
    "    img1, img2 = test_dataset[0]  # 取一个示例\n",
    "    img1 = img1.unsqueeze(0).to(device)\n",
    "    img2 = img2.unsqueeze(0).to(device)  # 增加批量维度\n",
    "    with torch.no_grad():\n",
    "        cyclegan.G_AB.eval()\n",
    "        outputA = cyclegan.G_AB(img1)\n",
    "        outputB = cyclegan.G_BA(img2)\n",
    "    # 将结果从 Tensor 转换回图片\n",
    "    outputA = outputA.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "    outputB = outputB.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "    img1 = img1.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "    img2 = img2.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    # 使用 matplotlib 显示图片\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
    "    axes[0].imshow(img1)\n",
    "    axes[0].set_title(\"Input Image 1\")\n",
    "    axes[1].imshow(img2)\n",
    "    axes[1].set_title(\"Input Image 2\")\n",
    "    axes[2].imshow(outputA)\n",
    "    axes[2].set_title(\"Predicted Image A\")\n",
    "    axes[3].imshow(outputB)\n",
    "    axes[3].set_title(\"Predicted Image B\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digitalFilm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

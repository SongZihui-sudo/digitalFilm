{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from torchvision.utils import save_image\n",
    "import itertools\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePairDataset(Dataset):\n",
    "    def __init__(self, folder1, folder2, pairs, transform=None, enhance_transform_1 = None, \n",
    "                 enhance_transform_2 = None, enhance_transform_3 = None):\n",
    "        self.folder1 = folder1\n",
    "        self.folder2 = folder2\n",
    "        self.pairs = pairs\n",
    "        self.transform = transform\n",
    "        self.enhance_transform_1 = enhance_transform_1\n",
    "        self.enhance_transform_2 = enhance_transform_2\n",
    "        self.enhance_transform_3 = enhance_transform_3\n",
    "        self.image_pairs = self.read_image_pairs()\n",
    "\n",
    "    def read_image_pairs(self):\n",
    "      image_pairs = []\n",
    "      for image_pair in tqdm.tqdm(self.pairs):\n",
    "        img1_path = os.path.join(self.folder1, image_pair[0])\n",
    "        img2_path = os.path.join(self.folder2, image_pair[1])\n",
    "        img1 = Image.open(img1_path).convert(\"RGB\")\n",
    "        img2 = Image.open(img2_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        image_pairs.append((img1, img2))\n",
    "      return image_pairs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      return self.image_pairs[idx][0], self.image_pairs[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 归一化\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "digital_dir = \"D:/data/禄来红外400/数码\"\n",
    "film_dir = \"D:/data/禄来红外400/样片\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(image1_dir, imgage2_dir):\n",
    "    # Get the list of files in both folders\n",
    "    images1 = sorted(os.listdir(image1_dir))\n",
    "    images2 = sorted(os.listdir(imgage2_dir))\n",
    "    \n",
    "    print(len(images1))\n",
    "    print(len(images2))\n",
    "\n",
    "    # Ensure the number of files match\n",
    "    if len(images1) != len(images2):\n",
    "        raise ValueError(\"The two folders must have the same number of images.\")\n",
    "\n",
    "    # Create pairs of images (file1, file2)\n",
    "    pairs = list(zip(images1, images2))\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n",
      "242\n"
     ]
    }
   ],
   "source": [
    "pairs = make_pairs(digital_dir, film_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 242/242 [00:23<00:00, 10.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = ImagePairDataset(digital_dir, film_dir, pairs, transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1. 定义生成器 (ResNet Generator)\n",
    "# ============================\n",
    "class ResNetGenerator(nn.Module):\n",
    "    def __init__(self, input_nc=3, output_nc=3, num_residual_blocks=9):\n",
    "        super(ResNetGenerator, self).__init__()\n",
    "        \n",
    "        # 使用预训练的 ResNet-18\n",
    "        resnet = models.resnet18()\n",
    "        resnet.load_state_dict(torch.load(\"./resnet18-5c106cde.pth\", weights_only=False))\n",
    "        \n",
    "        # 编码器：获取 ResNet 的各层输出，构造 U-Net 风格的跳跃连接\n",
    "        self.input_conv = nn.Sequential(\n",
    "            resnet.conv1,   # 输出: 64通道, H/2 x W/2\n",
    "            resnet.bn1,\n",
    "            resnet.relu\n",
    "        )\n",
    "        self.maxpool = resnet.maxpool   # H/4 x W/4\n",
    "        self.encoder1 = resnet.layer1   # 输出: 64通道, H/4 x W/4\n",
    "        self.encoder2 = resnet.layer2   # 输出: 128通道, H/8 x W/8\n",
    "        self.encoder3 = resnet.layer3   # 输出: 256通道, H/16 x W/16\n",
    "        self.encoder4 = resnet.layer4   # 输出: 512通道, H/32 x W/32\n",
    "\n",
    "        # 解码器：使用上采样（双线性插值）+ 卷积来恢复细节，并与跳跃连接特征融合\n",
    "        self.up1 = self._up_block(512, 256)  # H/32 -> H/16\n",
    "        self.up2 = self._up_block(512, 128)  # 256(from up1)+256(encoder3) -> H/16 -> H/8\n",
    "        self.up3 = self._up_block(256, 64)   # 128(from up2)+128(encoder2) -> H/8 -> H/4\n",
    "        self.up4 = self._up_block(128, 64)   # 64(from up3)+64(encoder1) -> H/4 -> H/2\n",
    "        self.up5 = self._up_block(128, 64)   # 64(from up4)+64(from input_conv) -> H/2 -> H\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, output_nc, kernel_size=1)\n",
    "\n",
    "    def _up_block(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        上采样块：先上采样（双线性），再卷积+ReLU\n",
    "        \"\"\"\n",
    "        block = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        return block\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x.shape[2]\n",
    "        w = x.shape[3]\n",
    "\n",
    "        # 编码器部分\n",
    "        x0 = self.input_conv(x)       # x0: [B, 64, H/2, W/2]\n",
    "        x1 = self.maxpool(x0)         # x1: [B, 64, H/4, W/4]\n",
    "        x1 = self.encoder1(x1)        # x1: [B, 64, H/4, W/4]\n",
    "        x2 = self.encoder2(x1)        # x2: [B, 128, H/8, W/8]\n",
    "        x3 = self.encoder3(x2)        # x3: [B, 256, H/16, W/16]\n",
    "        x4 = self.encoder4(x3)        # x4: [B, 512, H/32, W/32]\n",
    "\n",
    "        # 解码器部分：逐步上采样并与编码器对应层特征拼接\n",
    "        d1 = self.up1(x4)             # d1: [B, 256, H/16, W/16]\n",
    "        # 拼接 x3 (256通道)\n",
    "        d1 = torch.cat([d1, x3], dim=1)  # [B, 256+256=512, H/16, W/16]\n",
    "\n",
    "        d2 = self.up2(d1)             # d2: [B, 128, H/8, W/8]\n",
    "        # 拼接 x2 (128通道)\n",
    "        d2 = torch.cat([d2, x2], dim=1)  # [B, 128+128=256, H/8, W/8]\n",
    "\n",
    "        d3 = self.up3(d2)             # d3: [B, 64, H/4, W/4]\n",
    "        # 拼接 x1 (64通道)\n",
    "        d3 = torch.cat([d3, x1], dim=1)  # [B, 64+64=128, H/4, W/4]\n",
    "\n",
    "        d4 = self.up4(d3)             # d4: [B, 64, H/2, W/2]\n",
    "        # 拼接 x0 (64通道)\n",
    "        d4 = torch.cat([d4, x0], dim=1)  # [B, 64+64=128, H/2, W/2]\n",
    "\n",
    "        d5 = self.up5(d4)             # d5: [B, 64, H, W]\n",
    "\n",
    "        out = self.final_conv(d5)     # out: [B, output_nc, H, W]\n",
    "        out = torch.tanh(out)\n",
    "\n",
    "        out = torch.nn.functional.interpolate(out, size = (h, w), mode=\"bilinear\")\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 3. 定义判别器 (PatchGAN)\n",
    "# ============================\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc):\n",
    "        \"\"\"\n",
    "        使用 70×70 PatchGAN 判别器\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        model = [\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        in_channels = 64\n",
    "        out_channels = in_channels * 2\n",
    "        # 增加几层卷积\n",
    "        for _ in range(3):\n",
    "            model += [\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "            out_channels = in_channels * 2\n",
    "\n",
    "        # 最后一层卷积\n",
    "        model += [\n",
    "            nn.Conv2d(in_channels, 1, kernel_size=4, padding=1)\n",
    "        ]\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 4. 定义 CycleGAN 模型类（加入 AMP 支持）\n",
    "# ============================\n",
    "class CycleGAN:\n",
    "    def __init__(self, device, G_AB = \"\", G_BA = \"\", D_A = \"\", D_B = \"\"):\n",
    "        self.device = device\n",
    "\n",
    "        # 初始化两个生成器：G_AB（数码→胶片）、G_BA（胶片→数码）\n",
    "        self.G_AB = ResNetGenerator(3, 3).to(device)\n",
    "        if G_AB != \"\":\n",
    "            self.G_AB.load_state_dict(torch.load(G_AB))\n",
    "        self.G_BA = ResNetGenerator(3, 3).to(device)\n",
    "        if G_BA != \"\":\n",
    "            self.G_BA.load_state_dict(torch.load(G_BA))\n",
    "        # 初始化两个判别器：D_A（判别真实数码图像）、D_B（判别真实胶片图像）\n",
    "        self.D_A = Discriminator(3).to(device)\n",
    "        if D_A != \"\":\n",
    "            self.D_A.load_state_dict(torch.load(D_A))\n",
    "        self.D_B = Discriminator(3).to(device)\n",
    "        if D_B != \"\":\n",
    "            self.D_B.load_state_dict(torch.load(D_B))\n",
    "        # 定义损失函数：对抗损失、循环一致性损失、身份损失\n",
    "        self.criterion_GAN = nn.MSELoss().to(device)\n",
    "        self.criterion_cycle = nn.L1Loss().to(device)\n",
    "        self.criterion_identity = nn.L1Loss().to(device)\n",
    "\n",
    "        # 优化器（两个生成器共用一个优化器）\n",
    "        self.optimizer_G = optim.Adam(itertools.chain(self.G_AB.parameters(), self.G_BA.parameters()),\n",
    "                                    lr=0.0002, betas=(0.5, 0.999))\n",
    "        self.optimizer_D_A = optim.Adam(self.D_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        self.optimizer_D_B = optim.Adam(self.D_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "        # 创建 AMP GradScaler 对象\n",
    "        self.scaler_G = torch.amp.GradScaler(device=device)\n",
    "        self.scaler_D_A = torch.amp.GradScaler(device=device)\n",
    "        self.scaler_D_B = torch.amp.GradScaler(device=device)\n",
    "\n",
    "    def set_input(self, real_A, real_B):\n",
    "        self.real_A = real_A.to(self.device)\n",
    "        self.real_B = real_B.to(self.device)\n",
    "\n",
    "    def forward(self):\n",
    "        # A→B→A\n",
    "        self.fake_B = self.G_AB(self.real_A)\n",
    "        self.rec_A = self.G_BA(self.fake_B)\n",
    "        # B→A→B\n",
    "        self.fake_A = self.G_BA(self.real_B)\n",
    "        self.rec_B = self.G_AB(self.fake_A)\n",
    "\n",
    "    def backward_G(self):\n",
    "        # 身份损失：要求生成器在目标域图像上保持不变\n",
    "        self.idt_A = self.G_BA(self.real_A)\n",
    "        self.loss_idt_A = self.criterion_identity(self.idt_A, self.real_A) * 5.0\n",
    "        self.idt_B = self.G_AB(self.real_B)\n",
    "        self.loss_idt_B = self.criterion_identity(self.idt_B, self.real_B) * 5.0\n",
    "\n",
    "        # 对抗损失\n",
    "        pred_fake_B = self.D_B(self.fake_B)\n",
    "        target_real = torch.ones_like(pred_fake_B, device=self.device)\n",
    "        loss_GAN_AB = self.criterion_GAN(pred_fake_B, target_real)\n",
    "\n",
    "        pred_fake_A = self.D_A(self.fake_A)\n",
    "        target_real = torch.ones_like(pred_fake_A, device=self.device)\n",
    "        loss_GAN_BA = self.criterion_GAN(pred_fake_A, target_real)\n",
    "\n",
    "        # 循环一致性损失\n",
    "        loss_cycle_A = self.criterion_cycle(self.rec_A, self.real_A) * 10.0\n",
    "        loss_cycle_B = self.criterion_cycle(self.rec_B, self.real_B) * 10.0\n",
    "\n",
    "        # 总生成器损失（仅保存，不调用 backward）\n",
    "        self.loss_G = self.loss_idt_A + self.loss_idt_B + loss_GAN_AB + loss_GAN_BA + loss_cycle_A + loss_cycle_B\n",
    "\n",
    "    def backward_D_basic(self, netD, real, fake):\n",
    "        target_real = torch.ones_like(netD(real), device=self.device)\n",
    "        target_fake = torch.zeros_like(netD(fake.detach()), device=self.device)\n",
    "        loss_real = self.criterion_GAN(netD(real), target_real)\n",
    "        loss_fake = self.criterion_GAN(netD(fake.detach()), target_fake)\n",
    "        loss_D = (loss_real + loss_fake) * 0.5\n",
    "        return loss_D\n",
    "\n",
    "    def backward_D_A(self):\n",
    "        self.loss_D_A = self.backward_D_basic(self.D_A, self.real_A, self.fake_A)\n",
    "\n",
    "    def backward_D_B(self):\n",
    "        self.loss_D_B = self.backward_D_basic(self.D_B, self.real_B, self.fake_B)\n",
    "\n",
    "    def optimize_parameters(self):\n",
    "        # --------------------------\n",
    "        # 更新生成器\n",
    "        # --------------------------\n",
    "        self.optimizer_G.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            self.forward()       # 生成 fake_B、fake_A、rec_A、rec_B\n",
    "            self.backward_G()    # 计算 self.loss_G\n",
    "        self.scaler_G.scale(self.loss_G).backward()\n",
    "        self.scaler_G.step(self.optimizer_G)\n",
    "        self.scaler_G.update()\n",
    "\n",
    "        # --------------------------\n",
    "        # 更新判别器 D_A\n",
    "        # --------------------------\n",
    "        self.optimizer_D_A.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            self.backward_D_A()  # 计算 self.loss_D_A\n",
    "        self.scaler_D_A.scale(self.loss_D_A).backward()\n",
    "        self.scaler_D_A.step(self.optimizer_D_A)\n",
    "        self.scaler_D_A.update()\n",
    "\n",
    "        # --------------------------\n",
    "        # 更新判别器 D_B\n",
    "        # --------------------------\n",
    "        self.optimizer_D_B.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            self.backward_D_B()  # 计算 self.loss_D_B\n",
    "        self.scaler_D_B.scale(self.loss_D_B).backward()\n",
    "        self.scaler_D_B.step(self.optimizer_D_B)\n",
    "        self.scaler_D_B.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 5. 训练循环函数\n",
    "# ============================\n",
    "def train(cyclegan, dataloader, num_epochs=200, save_interval=10, pre_epoch = 0):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data_A, data_B) in enumerate(dataloader):\n",
    "            real_A = data_A\n",
    "            real_B = data_B\n",
    "            cyclegan.set_input(real_A, real_B)\n",
    "            cyclegan.optimize_parameters()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1 + pre_epoch}/{num_epochs + pre_epoch}] Batch [{i}] | \"\n",
    "                      f\"Loss_G: {cyclegan.loss_G.item():.4f} | \"\n",
    "                      f\"Loss_D_A: {cyclegan.loss_D_A.item():.4f} | \"\n",
    "                      f\"Loss_D_B: {cyclegan.loss_D_B.item():.4f}\")\n",
    "\n",
    "        # 定期保存输出结果及模型\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            os.makedirs(\"output\", exist_ok=True)\n",
    "            with torch.no_grad():\n",
    "                fake_B = cyclegan.G_AB(real_A.cuda())\n",
    "                fake_A = cyclegan.G_BA(real_B.cuda())\n",
    "            save_image(fake_B, f\"output/fake_B_epoch_{epoch+1+pre_epoch}.png\", normalize=True)\n",
    "            save_image(fake_A, f\"output/fake_A_epoch_{epoch+1+pre_epoch}.png\", normalize=True)\n",
    "            torch.save(cyclegan.G_AB.state_dict(), f\"output/G_AB_epoch_{epoch+1+pre_epoch}.pth\")\n",
    "            torch.save(cyclegan.G_BA.state_dict(), f\"output/G_BA_epoch_{epoch+1+pre_epoch}.pth\")\n",
    "            torch.save(cyclegan.D_A.state_dict(), f\"output/D_A_epoch_{epoch+1+pre_epoch}.pth\")\n",
    "            torch.save(cyclegan.D_B.state_dict(), f\"output/D_B_epoch_{epoch+1+pre_epoch}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 CycleGAN 模型\n",
    "cyclegan = CycleGAN(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13012\\3913547452.py:34: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler_G = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13012\\3913547452.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler_D_A = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13012\\3913547452.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler_D_B = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "cyclegan = CycleGAN(device, G_AB=\"output/G_AB_epoch_200.pth\", \n",
    "                    G_BA=\"output/G_BA_epoch_200.pth\", \n",
    "                    D_A=\"output/D_A_epoch_200.pth\", \n",
    "                    D_B=\"output/D_B_epoch_200.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12036\\251787170.py:92: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12036\\251787170.py:103: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12036\\251787170.py:113: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] Batch [0] | Loss_G: 17.1859 | Loss_D_A: 0.7276 | Loss_D_B: 0.6201\n",
      "Epoch [2/200] Batch [0] | Loss_G: 15.1006 | Loss_D_A: 0.4649 | Loss_D_B: 0.3228\n",
      "Epoch [3/200] Batch [0] | Loss_G: 10.0335 | Loss_D_A: 0.2056 | Loss_D_B: 0.2335\n",
      "Epoch [4/200] Batch [0] | Loss_G: 7.4754 | Loss_D_A: 0.1524 | Loss_D_B: 0.1834\n",
      "Epoch [5/200] Batch [0] | Loss_G: 6.4414 | Loss_D_A: 0.1901 | Loss_D_B: 0.1219\n",
      "Epoch [6/200] Batch [0] | Loss_G: 5.3579 | Loss_D_A: 0.1497 | Loss_D_B: 0.1242\n",
      "Epoch [7/200] Batch [0] | Loss_G: 5.5880 | Loss_D_A: 0.1069 | Loss_D_B: 0.1188\n",
      "Epoch [8/200] Batch [0] | Loss_G: 6.0971 | Loss_D_A: 0.2743 | Loss_D_B: 0.1996\n",
      "Epoch [9/200] Batch [0] | Loss_G: 5.3487 | Loss_D_A: 0.1404 | Loss_D_B: 0.0966\n",
      "Epoch [10/200] Batch [0] | Loss_G: 4.8820 | Loss_D_A: 0.0975 | Loss_D_B: 0.0938\n",
      "Epoch [11/200] Batch [0] | Loss_G: 6.0928 | Loss_D_A: 0.2463 | Loss_D_B: 0.3102\n",
      "Epoch [12/200] Batch [0] | Loss_G: 4.6406 | Loss_D_A: 0.1275 | Loss_D_B: 0.1369\n",
      "Epoch [13/200] Batch [0] | Loss_G: 5.0668 | Loss_D_A: 0.2264 | Loss_D_B: 0.0961\n",
      "Epoch [14/200] Batch [0] | Loss_G: 4.6059 | Loss_D_A: 0.1169 | Loss_D_B: 0.0987\n",
      "Epoch [15/200] Batch [0] | Loss_G: 4.7897 | Loss_D_A: 0.1795 | Loss_D_B: 0.1860\n",
      "Epoch [16/200] Batch [0] | Loss_G: 3.9940 | Loss_D_A: 0.1858 | Loss_D_B: 0.1179\n",
      "Epoch [17/200] Batch [0] | Loss_G: 5.2410 | Loss_D_A: 0.1362 | Loss_D_B: 0.1926\n",
      "Epoch [18/200] Batch [0] | Loss_G: 3.8781 | Loss_D_A: 0.1424 | Loss_D_B: 0.0996\n",
      "Epoch [19/200] Batch [0] | Loss_G: 5.6736 | Loss_D_A: 0.1893 | Loss_D_B: 0.1340\n",
      "Epoch [20/200] Batch [0] | Loss_G: 3.7706 | Loss_D_A: 0.1715 | Loss_D_B: 0.2155\n",
      "Epoch [21/200] Batch [0] | Loss_G: 4.2646 | Loss_D_A: 0.2124 | Loss_D_B: 0.1044\n",
      "Epoch [22/200] Batch [0] | Loss_G: 4.3542 | Loss_D_A: 0.0971 | Loss_D_B: 0.1049\n",
      "Epoch [23/200] Batch [0] | Loss_G: 4.4802 | Loss_D_A: 0.1473 | Loss_D_B: 0.0760\n",
      "Epoch [24/200] Batch [0] | Loss_G: 4.4105 | Loss_D_A: 0.1562 | Loss_D_B: 0.1640\n",
      "Epoch [25/200] Batch [0] | Loss_G: 4.3210 | Loss_D_A: 0.1294 | Loss_D_B: 0.4070\n",
      "Epoch [26/200] Batch [0] | Loss_G: 3.9166 | Loss_D_A: 0.1144 | Loss_D_B: 0.1114\n",
      "Epoch [27/200] Batch [0] | Loss_G: 4.7634 | Loss_D_A: 0.1176 | Loss_D_B: 0.1246\n",
      "Epoch [28/200] Batch [0] | Loss_G: 4.9181 | Loss_D_A: 0.4234 | Loss_D_B: 0.0817\n",
      "Epoch [29/200] Batch [0] | Loss_G: 3.4615 | Loss_D_A: 0.1355 | Loss_D_B: 0.1331\n",
      "Epoch [30/200] Batch [0] | Loss_G: 4.0066 | Loss_D_A: 0.1053 | Loss_D_B: 0.0931\n",
      "Epoch [31/200] Batch [0] | Loss_G: 4.0428 | Loss_D_A: 0.1095 | Loss_D_B: 0.0721\n",
      "Epoch [32/200] Batch [0] | Loss_G: 3.6785 | Loss_D_A: 0.0979 | Loss_D_B: 0.1075\n",
      "Epoch [33/200] Batch [0] | Loss_G: 4.5355 | Loss_D_A: 0.0850 | Loss_D_B: 0.1257\n",
      "Epoch [34/200] Batch [0] | Loss_G: 3.7173 | Loss_D_A: 0.1459 | Loss_D_B: 0.0758\n",
      "Epoch [35/200] Batch [0] | Loss_G: 3.6757 | Loss_D_A: 0.1549 | Loss_D_B: 0.0845\n",
      "Epoch [36/200] Batch [0] | Loss_G: 3.9060 | Loss_D_A: 0.0890 | Loss_D_B: 0.0807\n",
      "Epoch [37/200] Batch [0] | Loss_G: 4.2682 | Loss_D_A: 0.0981 | Loss_D_B: 0.0472\n",
      "Epoch [38/200] Batch [0] | Loss_G: 4.8984 | Loss_D_A: 0.1219 | Loss_D_B: 0.0753\n",
      "Epoch [39/200] Batch [0] | Loss_G: 5.1091 | Loss_D_A: 0.0641 | Loss_D_B: 0.2897\n",
      "Epoch [40/200] Batch [0] | Loss_G: 3.6528 | Loss_D_A: 0.1217 | Loss_D_B: 0.1132\n",
      "Epoch [41/200] Batch [0] | Loss_G: 4.5504 | Loss_D_A: 0.0700 | Loss_D_B: 0.0378\n",
      "Epoch [42/200] Batch [0] | Loss_G: 4.8536 | Loss_D_A: 0.0503 | Loss_D_B: 0.1044\n",
      "Epoch [43/200] Batch [0] | Loss_G: 4.7496 | Loss_D_A: 0.1088 | Loss_D_B: 0.0532\n",
      "Epoch [44/200] Batch [0] | Loss_G: 3.4218 | Loss_D_A: 0.1366 | Loss_D_B: 0.1968\n",
      "Epoch [45/200] Batch [0] | Loss_G: 4.1239 | Loss_D_A: 0.0926 | Loss_D_B: 0.1042\n",
      "Epoch [46/200] Batch [0] | Loss_G: 4.0854 | Loss_D_A: 0.0566 | Loss_D_B: 0.0562\n",
      "Epoch [47/200] Batch [0] | Loss_G: 4.1778 | Loss_D_A: 0.0566 | Loss_D_B: 0.0466\n",
      "Epoch [48/200] Batch [0] | Loss_G: 4.3551 | Loss_D_A: 0.1106 | Loss_D_B: 0.1485\n",
      "Epoch [49/200] Batch [0] | Loss_G: 4.6756 | Loss_D_A: 0.0885 | Loss_D_B: 0.0467\n",
      "Epoch [50/200] Batch [0] | Loss_G: 4.7629 | Loss_D_A: 0.2289 | Loss_D_B: 0.0757\n",
      "Epoch [51/200] Batch [0] | Loss_G: 4.7254 | Loss_D_A: 0.0693 | Loss_D_B: 0.0938\n",
      "Epoch [52/200] Batch [0] | Loss_G: 4.0762 | Loss_D_A: 0.0900 | Loss_D_B: 0.0560\n",
      "Epoch [53/200] Batch [0] | Loss_G: 4.8797 | Loss_D_A: 0.0463 | Loss_D_B: 0.0571\n",
      "Epoch [54/200] Batch [0] | Loss_G: 3.3165 | Loss_D_A: 0.2139 | Loss_D_B: 0.1562\n",
      "Epoch [55/200] Batch [0] | Loss_G: 3.8541 | Loss_D_A: 0.0819 | Loss_D_B: 0.0634\n",
      "Epoch [56/200] Batch [0] | Loss_G: 4.0412 | Loss_D_A: 0.0602 | Loss_D_B: 0.0566\n",
      "Epoch [57/200] Batch [0] | Loss_G: 4.1556 | Loss_D_A: 0.0751 | Loss_D_B: 0.0532\n",
      "Epoch [58/200] Batch [0] | Loss_G: 4.1384 | Loss_D_A: 0.0640 | Loss_D_B: 0.0696\n",
      "Epoch [59/200] Batch [0] | Loss_G: 4.1724 | Loss_D_A: 0.0612 | Loss_D_B: 0.1000\n",
      "Epoch [60/200] Batch [0] | Loss_G: 4.5643 | Loss_D_A: 0.0608 | Loss_D_B: 0.0501\n",
      "Epoch [61/200] Batch [0] | Loss_G: 3.6765 | Loss_D_A: 0.0873 | Loss_D_B: 0.1265\n",
      "Epoch [62/200] Batch [0] | Loss_G: 4.3078 | Loss_D_A: 0.1248 | Loss_D_B: 0.2564\n",
      "Epoch [63/200] Batch [0] | Loss_G: 4.3304 | Loss_D_A: 0.0802 | Loss_D_B: 0.0595\n",
      "Epoch [64/200] Batch [0] | Loss_G: 4.7142 | Loss_D_A: 0.0500 | Loss_D_B: 0.0466\n",
      "Epoch [65/200] Batch [0] | Loss_G: 4.3723 | Loss_D_A: 0.0632 | Loss_D_B: 0.0287\n",
      "Epoch [66/200] Batch [0] | Loss_G: 4.6758 | Loss_D_A: 0.1963 | Loss_D_B: 0.0347\n",
      "Epoch [67/200] Batch [0] | Loss_G: 4.1153 | Loss_D_A: 0.0929 | Loss_D_B: 0.0333\n",
      "Epoch [68/200] Batch [0] | Loss_G: 4.1911 | Loss_D_A: 0.0479 | Loss_D_B: 0.0421\n",
      "Epoch [69/200] Batch [0] | Loss_G: 4.9370 | Loss_D_A: 0.0505 | Loss_D_B: 0.1163\n",
      "Epoch [70/200] Batch [0] | Loss_G: 4.6625 | Loss_D_A: 0.0537 | Loss_D_B: 0.0296\n",
      "Epoch [71/200] Batch [0] | Loss_G: 5.1606 | Loss_D_A: 0.1603 | Loss_D_B: 0.0637\n",
      "Epoch [72/200] Batch [0] | Loss_G: 5.0250 | Loss_D_A: 0.0444 | Loss_D_B: 0.1149\n",
      "Epoch [73/200] Batch [0] | Loss_G: 3.8462 | Loss_D_A: 0.0420 | Loss_D_B: 0.0422\n",
      "Epoch [74/200] Batch [0] | Loss_G: 3.6178 | Loss_D_A: 0.0369 | Loss_D_B: 0.1487\n",
      "Epoch [75/200] Batch [0] | Loss_G: 3.9008 | Loss_D_A: 0.0454 | Loss_D_B: 0.0531\n",
      "Epoch [76/200] Batch [0] | Loss_G: 3.8984 | Loss_D_A: 0.3058 | Loss_D_B: 0.0394\n",
      "Epoch [77/200] Batch [0] | Loss_G: 4.3775 | Loss_D_A: 0.5071 | Loss_D_B: 0.1943\n",
      "Epoch [78/200] Batch [0] | Loss_G: 3.7437 | Loss_D_A: 0.2303 | Loss_D_B: 0.0422\n",
      "Epoch [79/200] Batch [0] | Loss_G: 4.0030 | Loss_D_A: 0.2297 | Loss_D_B: 0.0370\n",
      "Epoch [80/200] Batch [0] | Loss_G: 4.7458 | Loss_D_A: 0.2181 | Loss_D_B: 0.2881\n",
      "Epoch [81/200] Batch [0] | Loss_G: 3.6699 | Loss_D_A: 0.2075 | Loss_D_B: 0.0413\n",
      "Epoch [82/200] Batch [0] | Loss_G: 3.5220 | Loss_D_A: 0.1995 | Loss_D_B: 0.0331\n",
      "Epoch [83/200] Batch [0] | Loss_G: 4.3281 | Loss_D_A: 0.1911 | Loss_D_B: 0.0596\n",
      "Epoch [84/200] Batch [0] | Loss_G: 3.8125 | Loss_D_A: 0.1948 | Loss_D_B: 0.0417\n",
      "Epoch [85/200] Batch [0] | Loss_G: 3.7875 | Loss_D_A: 0.1608 | Loss_D_B: 0.0354\n",
      "Epoch [86/200] Batch [0] | Loss_G: 3.8648 | Loss_D_A: 0.1463 | Loss_D_B: 0.0308\n",
      "Epoch [87/200] Batch [0] | Loss_G: 5.3558 | Loss_D_A: 0.1424 | Loss_D_B: 0.4079\n",
      "Epoch [88/200] Batch [0] | Loss_G: 3.8265 | Loss_D_A: 0.1548 | Loss_D_B: 0.6697\n",
      "Epoch [89/200] Batch [0] | Loss_G: 3.0892 | Loss_D_A: 0.1143 | Loss_D_B: 0.2429\n",
      "Epoch [90/200] Batch [0] | Loss_G: 3.1794 | Loss_D_A: 0.1350 | Loss_D_B: 0.2201\n",
      "Epoch [91/200] Batch [0] | Loss_G: 4.3428 | Loss_D_A: 0.1105 | Loss_D_B: 0.2039\n",
      "Epoch [92/200] Batch [0] | Loss_G: 4.0473 | Loss_D_A: 0.0846 | Loss_D_B: 0.1919\n",
      "Epoch [93/200] Batch [0] | Loss_G: 3.4359 | Loss_D_A: 0.1102 | Loss_D_B: 0.1903\n",
      "Epoch [94/200] Batch [0] | Loss_G: 3.4174 | Loss_D_A: 0.0789 | Loss_D_B: 0.1555\n",
      "Epoch [95/200] Batch [0] | Loss_G: 3.7186 | Loss_D_A: 0.0931 | Loss_D_B: 0.1799\n",
      "Epoch [96/200] Batch [0] | Loss_G: 3.9096 | Loss_D_A: 0.0961 | Loss_D_B: 0.1297\n",
      "Epoch [97/200] Batch [0] | Loss_G: 4.0010 | Loss_D_A: 0.0849 | Loss_D_B: 0.1216\n",
      "Epoch [98/200] Batch [0] | Loss_G: 3.9153 | Loss_D_A: 0.0838 | Loss_D_B: 0.0957\n",
      "Epoch [99/200] Batch [0] | Loss_G: 3.3193 | Loss_D_A: 0.1080 | Loss_D_B: 0.1165\n",
      "Epoch [100/200] Batch [0] | Loss_G: 4.0510 | Loss_D_A: 0.0760 | Loss_D_B: 0.1214\n",
      "Epoch [101/200] Batch [0] | Loss_G: 3.2930 | Loss_D_A: 0.1502 | Loss_D_B: 0.0815\n",
      "Epoch [102/200] Batch [0] | Loss_G: 3.9371 | Loss_D_A: 0.0856 | Loss_D_B: 0.0756\n",
      "Epoch [103/200] Batch [0] | Loss_G: 4.0727 | Loss_D_A: 0.0818 | Loss_D_B: 0.1503\n",
      "Epoch [104/200] Batch [0] | Loss_G: 3.9927 | Loss_D_A: 0.0636 | Loss_D_B: 0.0680\n",
      "Epoch [105/200] Batch [0] | Loss_G: 3.7776 | Loss_D_A: 0.0932 | Loss_D_B: 0.0601\n",
      "Epoch [106/200] Batch [0] | Loss_G: 4.1666 | Loss_D_A: 0.0712 | Loss_D_B: 0.1657\n",
      "Epoch [107/200] Batch [0] | Loss_G: 3.9629 | Loss_D_A: 0.0602 | Loss_D_B: 0.0715\n",
      "Epoch [108/200] Batch [0] | Loss_G: 4.0309 | Loss_D_A: 0.0491 | Loss_D_B: 0.0523\n",
      "Epoch [109/200] Batch [0] | Loss_G: 4.8746 | Loss_D_A: 0.3080 | Loss_D_B: 0.1436\n",
      "Epoch [110/200] Batch [0] | Loss_G: 3.7700 | Loss_D_A: 0.0650 | Loss_D_B: 0.0513\n",
      "Epoch [111/200] Batch [0] | Loss_G: 5.2290 | Loss_D_A: 0.0758 | Loss_D_B: 0.0862\n",
      "Epoch [112/200] Batch [0] | Loss_G: 4.0894 | Loss_D_A: 0.1030 | Loss_D_B: 0.1058\n",
      "Epoch [113/200] Batch [0] | Loss_G: 4.1344 | Loss_D_A: 0.0493 | Loss_D_B: 0.0986\n",
      "Epoch [114/200] Batch [0] | Loss_G: 4.1683 | Loss_D_A: 0.0589 | Loss_D_B: 0.1280\n",
      "Epoch [115/200] Batch [0] | Loss_G: 3.7829 | Loss_D_A: 0.1169 | Loss_D_B: 0.0721\n",
      "Epoch [116/200] Batch [0] | Loss_G: 3.9494 | Loss_D_A: 0.0571 | Loss_D_B: 0.0771\n",
      "Epoch [117/200] Batch [0] | Loss_G: 3.6076 | Loss_D_A: 0.0646 | Loss_D_B: 0.0687\n",
      "Epoch [118/200] Batch [0] | Loss_G: 4.0114 | Loss_D_A: 0.1091 | Loss_D_B: 0.1450\n",
      "Epoch [119/200] Batch [0] | Loss_G: 3.7745 | Loss_D_A: 0.0565 | Loss_D_B: 0.0420\n",
      "Epoch [120/200] Batch [0] | Loss_G: 3.7823 | Loss_D_A: 0.0498 | Loss_D_B: 0.1529\n",
      "Epoch [121/200] Batch [0] | Loss_G: 3.6709 | Loss_D_A: 0.0950 | Loss_D_B: 0.0817\n",
      "Epoch [122/200] Batch [0] | Loss_G: 4.1439 | Loss_D_A: 0.0910 | Loss_D_B: 0.2292\n",
      "Epoch [123/200] Batch [0] | Loss_G: 4.1845 | Loss_D_A: 0.0571 | Loss_D_B: 0.0697\n",
      "Epoch [124/200] Batch [0] | Loss_G: 5.2575 | Loss_D_A: 0.0737 | Loss_D_B: 0.0640\n",
      "Epoch [125/200] Batch [0] | Loss_G: 3.8726 | Loss_D_A: 0.1777 | Loss_D_B: 0.1293\n",
      "Epoch [126/200] Batch [0] | Loss_G: 3.6802 | Loss_D_A: 0.0895 | Loss_D_B: 0.0927\n",
      "Epoch [127/200] Batch [0] | Loss_G: 4.4510 | Loss_D_A: 0.0508 | Loss_D_B: 0.1124\n",
      "Epoch [128/200] Batch [0] | Loss_G: 4.0801 | Loss_D_A: 0.0495 | Loss_D_B: 0.0875\n",
      "Epoch [129/200] Batch [0] | Loss_G: 4.8605 | Loss_D_A: 0.0697 | Loss_D_B: 0.0611\n",
      "Epoch [130/200] Batch [0] | Loss_G: 3.5423 | Loss_D_A: 0.0648 | Loss_D_B: 0.0718\n",
      "Epoch [131/200] Batch [0] | Loss_G: 3.6831 | Loss_D_A: 0.0576 | Loss_D_B: 0.1063\n",
      "Epoch [132/200] Batch [0] | Loss_G: 3.7582 | Loss_D_A: 0.0527 | Loss_D_B: 0.0604\n",
      "Epoch [133/200] Batch [0] | Loss_G: 4.1473 | Loss_D_A: 0.0577 | Loss_D_B: 0.0653\n",
      "Epoch [134/200] Batch [0] | Loss_G: 4.0248 | Loss_D_A: 0.0647 | Loss_D_B: 0.0791\n",
      "Epoch [135/200] Batch [0] | Loss_G: 4.2612 | Loss_D_A: 0.0649 | Loss_D_B: 0.1061\n",
      "Epoch [136/200] Batch [0] | Loss_G: 4.0419 | Loss_D_A: 0.1345 | Loss_D_B: 0.1673\n",
      "Epoch [137/200] Batch [0] | Loss_G: 4.5804 | Loss_D_A: 0.0427 | Loss_D_B: 0.0723\n",
      "Epoch [138/200] Batch [0] | Loss_G: 3.9236 | Loss_D_A: 0.0536 | Loss_D_B: 0.0601\n",
      "Epoch [139/200] Batch [0] | Loss_G: 4.2045 | Loss_D_A: 0.0381 | Loss_D_B: 0.0593\n",
      "Epoch [140/200] Batch [0] | Loss_G: 4.9651 | Loss_D_A: 0.3353 | Loss_D_B: 0.1042\n",
      "Epoch [141/200] Batch [0] | Loss_G: 4.4473 | Loss_D_A: 0.0800 | Loss_D_B: 0.1220\n",
      "Epoch [142/200] Batch [0] | Loss_G: 3.4260 | Loss_D_A: 0.0562 | Loss_D_B: 0.1217\n",
      "Epoch [143/200] Batch [0] | Loss_G: 4.5477 | Loss_D_A: 0.0431 | Loss_D_B: 0.2384\n",
      "Epoch [144/200] Batch [0] | Loss_G: 3.9556 | Loss_D_A: 0.0473 | Loss_D_B: 0.0699\n",
      "Epoch [145/200] Batch [0] | Loss_G: 3.9049 | Loss_D_A: 0.0481 | Loss_D_B: 0.0525\n",
      "Epoch [146/200] Batch [0] | Loss_G: 4.5908 | Loss_D_A: 0.0417 | Loss_D_B: 0.1120\n",
      "Epoch [147/200] Batch [0] | Loss_G: 4.1420 | Loss_D_A: 0.0830 | Loss_D_B: 0.0793\n",
      "Epoch [148/200] Batch [0] | Loss_G: 4.9289 | Loss_D_A: 0.0342 | Loss_D_B: 0.1227\n",
      "Epoch [149/200] Batch [0] | Loss_G: 5.2137 | Loss_D_A: 0.0452 | Loss_D_B: 0.0745\n",
      "Epoch [150/200] Batch [0] | Loss_G: 4.5468 | Loss_D_A: 0.0623 | Loss_D_B: 0.0615\n",
      "Epoch [151/200] Batch [0] | Loss_G: 4.5314 | Loss_D_A: 0.1384 | Loss_D_B: 0.2448\n",
      "Epoch [152/200] Batch [0] | Loss_G: 3.7052 | Loss_D_A: 0.0523 | Loss_D_B: 0.0951\n",
      "Epoch [153/200] Batch [0] | Loss_G: 3.7451 | Loss_D_A: 0.0693 | Loss_D_B: 0.0585\n",
      "Epoch [154/200] Batch [0] | Loss_G: 3.6402 | Loss_D_A: 0.2034 | Loss_D_B: 0.0618\n",
      "Epoch [155/200] Batch [0] | Loss_G: 4.5242 | Loss_D_A: 0.0706 | Loss_D_B: 0.1447\n",
      "Epoch [156/200] Batch [0] | Loss_G: 3.9837 | Loss_D_A: 0.0449 | Loss_D_B: 0.0784\n",
      "Epoch [157/200] Batch [0] | Loss_G: 3.5515 | Loss_D_A: 0.0747 | Loss_D_B: 0.0766\n",
      "Epoch [158/200] Batch [0] | Loss_G: 4.3457 | Loss_D_A: 0.1140 | Loss_D_B: 0.1143\n",
      "Epoch [159/200] Batch [0] | Loss_G: 3.8783 | Loss_D_A: 0.0813 | Loss_D_B: 0.0569\n",
      "Epoch [160/200] Batch [0] | Loss_G: 3.9415 | Loss_D_A: 0.0445 | Loss_D_B: 0.0613\n",
      "Epoch [161/200] Batch [0] | Loss_G: 3.4927 | Loss_D_A: 0.0400 | Loss_D_B: 0.2034\n",
      "Epoch [162/200] Batch [0] | Loss_G: 3.9444 | Loss_D_A: 0.0628 | Loss_D_B: 0.1277\n",
      "Epoch [163/200] Batch [0] | Loss_G: 4.1869 | Loss_D_A: 0.0431 | Loss_D_B: 0.0622\n",
      "Epoch [164/200] Batch [0] | Loss_G: 4.5932 | Loss_D_A: 0.0459 | Loss_D_B: 0.1001\n",
      "Epoch [165/200] Batch [0] | Loss_G: 4.1419 | Loss_D_A: 0.0326 | Loss_D_B: 0.0580\n",
      "Epoch [166/200] Batch [0] | Loss_G: 3.9466 | Loss_D_A: 0.0641 | Loss_D_B: 0.0771\n",
      "Epoch [167/200] Batch [0] | Loss_G: 4.3251 | Loss_D_A: 0.1877 | Loss_D_B: 0.2149\n",
      "Epoch [168/200] Batch [0] | Loss_G: 4.1267 | Loss_D_A: 0.0498 | Loss_D_B: 0.0649\n",
      "Epoch [169/200] Batch [0] | Loss_G: 4.2948 | Loss_D_A: 0.0329 | Loss_D_B: 0.0381\n",
      "Epoch [170/200] Batch [0] | Loss_G: 3.3894 | Loss_D_A: 0.0922 | Loss_D_B: 0.1081\n",
      "Epoch [171/200] Batch [0] | Loss_G: 3.8960 | Loss_D_A: 0.1027 | Loss_D_B: 0.1682\n",
      "Epoch [172/200] Batch [0] | Loss_G: 4.0804 | Loss_D_A: 0.0319 | Loss_D_B: 0.0560\n",
      "Epoch [173/200] Batch [0] | Loss_G: 3.6115 | Loss_D_A: 0.0504 | Loss_D_B: 0.0421\n",
      "Epoch [174/200] Batch [0] | Loss_G: 3.8326 | Loss_D_A: 0.0543 | Loss_D_B: 0.0431\n",
      "Epoch [175/200] Batch [0] | Loss_G: 3.3255 | Loss_D_A: 0.1909 | Loss_D_B: 0.1544\n",
      "Epoch [176/200] Batch [0] | Loss_G: 4.5299 | Loss_D_A: 0.0464 | Loss_D_B: 0.0499\n",
      "Epoch [177/200] Batch [0] | Loss_G: 4.6325 | Loss_D_A: 0.0414 | Loss_D_B: 0.0962\n",
      "Epoch [178/200] Batch [0] | Loss_G: 4.2194 | Loss_D_A: 0.0867 | Loss_D_B: 0.0908\n",
      "Epoch [179/200] Batch [0] | Loss_G: 4.2015 | Loss_D_A: 0.0539 | Loss_D_B: 0.1589\n",
      "Epoch [180/200] Batch [0] | Loss_G: 6.7442 | Loss_D_A: 0.0621 | Loss_D_B: 2.4745\n",
      "Epoch [181/200] Batch [0] | Loss_G: 3.8748 | Loss_D_A: 0.0369 | Loss_D_B: 0.2467\n",
      "Epoch [182/200] Batch [0] | Loss_G: 2.9316 | Loss_D_A: 0.0907 | Loss_D_B: 0.2415\n",
      "Epoch [183/200] Batch [0] | Loss_G: 3.2675 | Loss_D_A: 0.0706 | Loss_D_B: 0.2414\n",
      "Epoch [184/200] Batch [0] | Loss_G: 3.3060 | Loss_D_A: 0.0515 | Loss_D_B: 0.2231\n",
      "Epoch [185/200] Batch [0] | Loss_G: 3.5665 | Loss_D_A: 0.0476 | Loss_D_B: 0.2237\n",
      "Epoch [186/200] Batch [0] | Loss_G: 3.4814 | Loss_D_A: 0.0377 | Loss_D_B: 0.2157\n",
      "Epoch [187/200] Batch [0] | Loss_G: 3.5538 | Loss_D_A: 0.1317 | Loss_D_B: 0.2141\n",
      "Epoch [188/200] Batch [0] | Loss_G: 3.4294 | Loss_D_A: 0.0433 | Loss_D_B: 0.2205\n",
      "Epoch [189/200] Batch [0] | Loss_G: 3.3569 | Loss_D_A: 0.0433 | Loss_D_B: 0.2005\n",
      "Epoch [190/200] Batch [0] | Loss_G: 3.3853 | Loss_D_A: 0.0431 | Loss_D_B: 0.1809\n",
      "Epoch [191/200] Batch [0] | Loss_G: 3.8016 | Loss_D_A: 0.0323 | Loss_D_B: 0.1865\n",
      "Epoch [192/200] Batch [0] | Loss_G: 3.4572 | Loss_D_A: 0.0360 | Loss_D_B: 0.2246\n",
      "Epoch [193/200] Batch [0] | Loss_G: 3.9813 | Loss_D_A: 0.0460 | Loss_D_B: 0.2080\n",
      "Epoch [194/200] Batch [0] | Loss_G: 3.3637 | Loss_D_A: 0.0831 | Loss_D_B: 0.2168\n",
      "Epoch [195/200] Batch [0] | Loss_G: 4.4186 | Loss_D_A: 0.0537 | Loss_D_B: 0.2296\n",
      "Epoch [196/200] Batch [0] | Loss_G: 3.8698 | Loss_D_A: 0.0374 | Loss_D_B: 0.2031\n",
      "Epoch [197/200] Batch [0] | Loss_G: 3.6167 | Loss_D_A: 0.0303 | Loss_D_B: 0.1991\n",
      "Epoch [198/200] Batch [0] | Loss_G: 3.8478 | Loss_D_A: 0.0839 | Loss_D_B: 0.1808\n",
      "Epoch [199/200] Batch [0] | Loss_G: 5.1869 | Loss_D_A: 0.0483 | Loss_D_B: 0.1461\n",
      "Epoch [200/200] Batch [0] | Loss_G: 4.1800 | Loss_D_A: 0.0250 | Loss_D_B: 0.1931\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "train(cyclegan, train_loader, num_epochs=200, pre_epoch = 0, save_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_1456\\3913547452.py:34: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler_G = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_1456\\3913547452.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler_D_A = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_1456\\3913547452.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler_D_B = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "cyclegan = CycleGAN(device, G_AB=\"output/G_AB_epoch_100.pth\", \n",
    "                    G_BA=\"output/G_BA_epoch_100.pth\", \n",
    "                    D_A=\"output/D_A_epoch_100.pth\", \n",
    "                    D_B=\"output/D_B_epoch_100.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 归一化\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_digital_dir = \"D:/data/柯达金200/微调数据集/测试照片\"\n",
    "test_film_dir = \"D:/data/柯达金200/微调数据集/测试胶片\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "pairs = make_pairs(test_digital_dir, test_film_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = ImagePairDataset(test_digital_dir, test_film_dir, pairs, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_1456\\1491647904.py:48: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  x = torch.nn.functional.upsample(x, size = (h, w), mode=\"bilinear\")\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.970074..0.9999682].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.99999946..0.99990547].\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# 测试阶段：展示结果\n",
    "with torch.no_grad():\n",
    "    img1, img2 = test_dataset[0]  # 取一个示例\n",
    "    img1 = img1.unsqueeze(0).to(device)\n",
    "    img2 = img2.unsqueeze(0).to(device)  # 增加批量维度\n",
    "    with torch.no_grad():\n",
    "        cyclegan.G_AB.eval()\n",
    "        outputA = cyclegan.G_AB(img1)\n",
    "        outputB = cyclegan.G_BA(img2)\n",
    "    # 将结果从 Tensor 转换回图片\n",
    "    outputA = outputA.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "    outputB = outputB.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "    img1 = img1.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "    img2 = img2.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    # 使用 matplotlib 显示图片\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
    "    axes[0].imshow(img1)\n",
    "    axes[0].set_title(\"Input Image 1\")\n",
    "    axes[1].imshow(img2)\n",
    "    axes[1].set_title(\"Input Image 2\")\n",
    "    axes[2].imshow(outputA)\n",
    "    axes[2].set_title(\"Predicted Image A\")\n",
    "    axes[3].imshow(outputB)\n",
    "    axes[3].set_title(\"Predicted Image B\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digitalFilm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
